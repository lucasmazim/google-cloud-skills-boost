{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ffd93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Task 1. Enable APIs\n",
    "In this section, let's enable the service networking API.\n",
    "\n",
    "To enable the Service Networking API, follow these steps:\n",
    "\n",
    "Type Service Networking API into the top search bar of the Google Cloud Console, choose the result as shown in the following image.\n",
    "\n",
    "serviceapi\n",
    "\n",
    "Click Enable.\n",
    "\n",
    "Task 2. Prepare VPC Network and Install Packages\n",
    "To reduce any network overhead that might lead to unnecessary increase in overhead latency, it is best to call the ANN endpoints from your VPC via a direct VPC Peering connection. The following section describes how to setup a VPC Peering connection.\n",
    "\n",
    "In the Google Cloud console, navigate to Vertex AI Workbench. In the top search bar of the Google Cloud console, enter Vertex AI Workbench, and click on the first result.\n",
    "Vertex AI\n",
    "\n",
    "Click on User managed notebooks and then click on Open JupyterLab for generative-ai-jupyterlab notebook.\n",
    "The JupyterLab will run in a new tab.\n",
    "\n",
    "Jupyter\n",
    "\n",
    "On the Launcher, under Notebook, click on Python 3 to open a new python notebook.\n",
    "\n",
    "Run the following code snippet in the first cell by clicking the play play button at the top or enter SHIFT+ENTER on your keyboard to execute the cell.\n",
    "\n",
    "PROJECT_ID = \"GCP Project ID\"\n",
    "NETWORK_NAME = \"Network Name\"\n",
    "PEERING_RANGE_NAME = \"VPC Peering Range Name\"\n",
    "\n",
    "# Reserve IP range\n",
    "! gcloud compute addresses create {PEERING_RANGE_NAME} --global --prefix-length=16 --network={NETWORK_NAME} --purpose=VPC_PEERING --project={PROJECT_ID} --description=\"peering range for cymbal demo\"\n",
    "\n",
    "# Set up peering with service networking\n",
    "! gcloud services vpc-peerings connect --service=servicenetworking.googleapis.com --network={NETWORK_NAME} --ranges={PEERING_RANGE_NAME} --project={PROJECT_ID}\n",
    "Copied!\n",
    "Download and install the latest (preview) version of the Vertex AI SDK for Python.\n",
    "! pip install -U git+https://github.com/googleapis/python-aiplatform.git@main --user\n",
    "Copied!\n",
    "Install the h5py to prepare sample dataset, and the grpcio-tools for querying against the index.\n",
    "! pip install -U grpcio-tools --user\n",
    "! pip install -U h5py --user\n",
    "! pip install proto-plus==1.24.0.dev1\n",
    "Copied!\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages.\n",
    "\n",
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "kernel restart image\n",
    "\n",
    "Task 3. Create a Cloud Storage Bucket\n",
    "Run the following code snippet to create a Cloud Storage Bucket in [Lab GCP Region] region.\n",
    "BUCKET_NAME = \"gs://GCP Project ID-aip\"\n",
    "REGION = \"Lab GCP Region\"\n",
    "PROJECT_ID = \"GCP Project ID\"\n",
    "NETWORK_NAME = \"Network Name\"\n",
    "Copied!\n",
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_NAME\n",
    "Copied!\n",
    "Finally, validate access to your Cloud Storage bucket by examining its contents:\n",
    "! gsutil ls -al $BUCKET_NAME\n",
    "Copied!\n",
    "Task 4. Import Libraries\n",
    "Import the Vertex AI (unified) client library into your Python environment.\n",
    "Run the following code snippet to import Vertex AI client library.\n",
    "# Upgrade protobuf to the latest version\n",
    "!pip install --upgrade protobuf\n",
    "\n",
    "import time\n",
    "import grpc\n",
    "import h5py\n",
    "from google.cloud import aiplatform_v1\n",
    "from google.protobuf import struct_pb2\n",
    "Copied!\n",
    "REGION = \"Lab GCP Region\"\n",
    "ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "NETWORK_NAME = \"Network Name\"\n",
    "\n",
    "\n",
    "AUTH_TOKEN = !gcloud auth print-access-token\n",
    "PROJECT_NUMBER = !gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = PROJECT_NUMBER[0]\n",
    "\n",
    "PARENT = \"projects/{}/locations/{}\".format(PROJECT_ID, REGION)\n",
    "\n",
    "print(\"ENDPOINT: {}\".format(ENDPOINT))\n",
    "print(\"PROJECT_ID: {}\".format(PROJECT_ID))\n",
    "print(\"REGION: {}\".format(REGION))\n",
    "\n",
    "!gcloud config set project {PROJECT_ID} --quiet\n",
    "!gcloud config set ai_platform/region {REGION} --quiet\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "Endpoint Result\n",
    "\n",
    "Task 5. Prepare the Data\n",
    "The GloVe dataset consists of a set of pre-trained embeddings. The embeddings are split into a \"train\" split, and a \"test\" split. We will create a vector search index from the \"train\" split, and use the embedding vectors in the \"test\" split as query vectors to test the vector search index.\n",
    "\n",
    "Note: While the data split uses the term \"train\", these are pre-trained embeddings and thus are ready to be indexed for search. The terms \"train\" and \"test\" split are used just to be consistent with usual machine learning terminology.\n",
    "Run the following code snippet to download the GloVe dataset.\n",
    "! gsutil cp gs://cloud-samples-data/vertex-ai/matching_engine/glove-100-angular.hdf5 .\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "Copied Response\n",
    "\n",
    "Read the data into memory.\n",
    "# The number of nearest neighbors to be retrieved from database for each query.\n",
    "k = 10\n",
    "\n",
    "h5 = h5py.File(\"glove-100-angular.hdf5\", \"r\")\n",
    "train = h5[\"train\"]\n",
    "test = h5[\"test\"]\n",
    "Copied!\n",
    "train[0]\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "Model Response\n",
    "\n",
    "Save the train split in JSONL format.\n",
    "# Add restricts to each data point, in this demo, we only add one namespace and the allowlist is set to be the same as the id.\n",
    "# Later on, we will demo how to return only the allowlisted data points.\n",
    "# Split datapoins into two groups 'a' and 'b'. The datapoint whose ids are even are in group 'a', otherwise are in group 'b'\n",
    "# We will demo how to configure the query to return up to k data points for each group.\n",
    "with open(\"glove100.json\", \"w\") as f:\n",
    "    for i in range(len(train)):\n",
    "        f.write('{\"id\":\"' + str(i) + '\",')\n",
    "        f.write('\"embedding\":[' + \",\".join(str(x) for x in train[i]) + \"],\")\n",
    "        f.write('\"restricts\":[{\"namespace\": \"class\", \"allow\": [\"' + str(i) + '\"]}],')\n",
    "        f.write('\"crowding_tag\":' + ('\"a\"' if i % 2 == 0 else '\"b\"') + \"}\")\n",
    "        f.write(\"\\n\")\n",
    "        if i >= 100:\n",
    "            break\n",
    "Copied!\n",
    "Upload the training data to Google Cloud Storage Bucket created earlier.\n",
    "# NOTE: Everything in this GCS DIR will be DELETED before uploading the data.\n",
    "\n",
    "! gsutil rm -rf {BUCKET_NAME}/*\n",
    "Copied!\n",
    "! gsutil cp glove100.json {BUCKET_NAME}/glove100.json\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "Copied Response\n",
    "\n",
    "List the newly added contents in the bucket.\n",
    "! gsutil ls {BUCKET_NAME}\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "List of Bucket Objects\n",
    "\n",
    "Task 6. Create Stream Update Index\n",
    "Run the following code snippet in the next cells to create a instance of the IndexServiceClient from the AI Platform (Unified) Python client library. This is used for interacting with AI Platform services related to indexes, such as creating and managing Approximate Nearest Neighbor (ANN)\n",
    "index_client = aiplatform_v1.IndexServiceClient(\n",
    "    client_options=dict(api_endpoint=ENDPOINT)\n",
    ")\n",
    "Copied!\n",
    "DIMENSIONS = 100\n",
    "DISPLAY_NAME = \"glove_100_1\"\n",
    "Copied!\n",
    "Let's define the configuration for creating an Approximate Nearest Neighbor (ANN) index.\n",
    "treeAhConfig = struct_pb2.Struct(\n",
    "    fields={\n",
    "        \"leafNodeEmbeddingCount\": struct_pb2.Value(number_value=500),\n",
    "        \"leafNodesToSearchPercent\": struct_pb2.Value(number_value=7),\n",
    "    }\n",
    ")\n",
    "\n",
    "algorithmConfig = struct_pb2.Struct(\n",
    "    fields={\"treeAhConfig\": struct_pb2.Value(struct_value=treeAhConfig)}\n",
    ")\n",
    "\n",
    "config = struct_pb2.Struct(\n",
    "    fields={\n",
    "        \"dimensions\": struct_pb2.Value(number_value=DIMENSIONS),\n",
    "        \"approximateNeighborsCount\": struct_pb2.Value(number_value=150),\n",
    "        \"distanceMeasureType\": struct_pb2.Value(string_value=\"DOT_PRODUCT_DISTANCE\"),\n",
    "        \"algorithmConfig\": struct_pb2.Value(struct_value=algorithmConfig),\n",
    "    }\n",
    ")\n",
    "\n",
    "metadata = struct_pb2.Struct(\n",
    "    fields={\n",
    "        \"config\": struct_pb2.Value(struct_value=config),\n",
    "        \"contentsDeltaUri\": struct_pb2.Value(string_value=BUCKET_NAME),\n",
    "    }\n",
    ")\n",
    "\n",
    "ann_index = {\n",
    "    \"display_name\": DISPLAY_NAME,\n",
    "    \"description\": \"Glove 100 ANN index\",\n",
    "    \"metadata\": struct_pb2.Value(struct_value=metadata),\n",
    "    \"index_update_method\": aiplatform_v1.Index.IndexUpdateMethod.STREAM_UPDATE,\n",
    "}\n",
    "Copied!\n",
    "Now, let's create an Approximate Nearest Neighbor (ANN) index using the specified configuration.\n",
    "ann_index = index_client.create_index(parent=PARENT, index=ann_index)\n",
    "Copied!\n",
    "Retrieve the result of previously executed ann_index.\n",
    "ann_index.result()\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "Ann Index Response\n",
    "\n",
    "Note It can take up to 15 minutes to be available.\n",
    "Finally, retrieve the name of the created index (ann_index) and store it in the variable INDEX_RESOURCE_NAME.\n",
    "INDEX_RESOURCE_NAME = ann_index.result().name\n",
    "INDEX_RESOURCE_NAME\n",
    "Copied!\n",
    "Task 7. Create an IndexEndpoint with VPC Network\n",
    "Run the following code snippet to initialize an AI Platform Index Endpoint Service client.\n",
    "index_endpoint_client = aiplatform_v1.IndexEndpointServiceClient(\n",
    "    client_options=dict(api_endpoint=ENDPOINT)\n",
    ")\n",
    "Copied!\n",
    "Define a configuration for an AI Platform Index Endpoint.\n",
    "VPC_NETWORK_NAME = \"projects/{}/global/networks/{}\".format(PROJECT_NUMBER, NETWORK_NAME)\n",
    "VPC_NETWORK_NAME\n",
    "Copied!\n",
    "index_endpoint = {\n",
    "    \"display_name\": \"index_endpoint_for_demo\",\n",
    "    \"network\": VPC_NETWORK_NAME,\n",
    "}\n",
    "Copied!\n",
    "Create an AI Platform Index Endpoint.\n",
    "r = index_endpoint_client.create_index_endpoint(\n",
    "    parent=PARENT, index_endpoint=index_endpoint\n",
    ")\n",
    "Copied!\n",
    "Retrieve the result of previous operation.\n",
    "r.result()\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "R Response\n",
    "\n",
    "Finally, retrieve the name of the created index endpoint and store it in the variable INDEX_ENDPOINT_NAME.\n",
    "INDEX_ENDPOINT_NAME = r.result().name\n",
    "INDEX_ENDPOINT_NAME\n",
    "Copied!\n",
    "Task 8. Deploy Stream Update Index\n",
    "Run the following code snippet in the next cells to deploy the index to an index endpoint.\n",
    "DEPLOYED_INDEX_ID = \"stream_update_glove_deployed\"\n",
    "Copied!\n",
    "deploy_ann_index = {\n",
    "    \"id\": DEPLOYED_INDEX_ID,\n",
    "    \"display_name\": DEPLOYED_INDEX_ID,\n",
    "    \"index\": INDEX_RESOURCE_NAME,\n",
    "}\n",
    "Copied!\n",
    "r = index_endpoint_client.deploy_index(\n",
    "    index_endpoint=INDEX_ENDPOINT_NAME, deployed_index=deploy_ann_index\n",
    ")\n",
    "Copied!\n",
    "Wait for the operation to complete.\n",
    "# Poll the operation until it's done successfullly.\n",
    "\n",
    "while True:\n",
    "    if r.done():\n",
    "        break\n",
    "    print(\"Poll the operation to deploy index...\")\n",
    "    time.sleep(60)\n",
    "     \n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "Poll Response\n",
    "\n",
    "Note: This operation might take upto 15 minutes to complete.\n",
    "Retrieve the result of the operation.\n",
    "r.result()\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "Deployed Index Result\n",
    "\n",
    "Task 9. Create Online Queries\n",
    "After you built your indexes, you may query against the deployed index through the online querying gRPC API (Match service) within the virtual machine instances from the same region.\n",
    "\n",
    "Run the following code snippet to create write match_service.proto locally. This is a Protocol Buffer (protobuf) definition for a service called MatchService.\n",
    "%%writefile match_service.proto\n",
    "\n",
    "syntax = \"proto3\";\n",
    "\n",
    "package google.cloud.aiplatform.container.v1;\n",
    "\n",
    "// MatchService is a Google managed service for efficient vector similarity\n",
    "// search at scale.\n",
    "service MatchService {\n",
    "  // Returns the nearest neighbors for the query. If it is a sharded\n",
    "  // deployment, calls the other shards and aggregates the responses.\n",
    "  rpc Match(MatchRequest) returns (MatchResponse) {}\n",
    "}\n",
    "\n",
    "// Parameters for a match query.\n",
    "message MatchRequest {\n",
    "  // The ID of the DeploydIndex that will serve the request.\n",
    "  // This MatchRequest is sent to a specific IndexEndpoint of the Control API,\n",
    "  // as per the IndexEndpoint.network. That IndexEndpoint also has\n",
    "  // IndexEndpoint.deployed_indexes, and each such index has an\n",
    "  // DeployedIndex.id field.\n",
    "  // The value of the field below must equal one of the DeployedIndex.id\n",
    "  // fields of the IndexEndpoint that is being called for this request.\n",
    "  string deployed_index_id = 1;\n",
    "\n",
    "  // The embedding values.\n",
    "  repeated float float_val = 2;\n",
    "\n",
    "  // The number of nearest neighbors to be retrieved from database for\n",
    "  // each query. If not set, will use the default from\n",
    "  // the service configuration.\n",
    "  int32 num_neighbors = 3;\n",
    "\n",
    "  // The list of restricts.\n",
    "  repeated Namespace restricts = 4;\n",
    "\n",
    "  // Crowding is a constraint on a neighbor list produced by nearest neighbor\n",
    "  // search requiring that no more than some value k' of the k neighbors\n",
    "  // returned have the same value of crowding_attribute.\n",
    "  // It's used for improving result diversity.\n",
    "  // This field is the maximum number of matches with the same crowding tag.\n",
    "  int32 per_crowding_attribute_num_neighbors = 5;\n",
    "\n",
    "  // The number of neighbors to find via approximate search before\n",
    "  // exact reordering is performed. If not set, the default value from scam\n",
    "  // config is used; if set, this value must be > 0.\n",
    "  int32 approx_num_neighbors = 6;\n",
    "\n",
    "  // The fraction of the number of leaves to search, set at query time allows\n",
    "  // user to tune search performance. This value increase result in both search\n",
    "  // accuracy and latency increase. The value should be between 0.0 and 1.0. If\n",
    "  // not set or set to 0.0, query uses the default value specified in\n",
    "  // NearestNeighborSearchConfig.TreeAHConfig.leaf_nodes_to_search_percent.\n",
    "  int32 leaf_nodes_to_search_percent_override = 7;\n",
    "}\n",
    "\n",
    "// Response of a match query.\n",
    "message MatchResponse {\n",
    "  message Neighbor {\n",
    "    // The ids of the matches.\n",
    "    string id = 1;\n",
    "\n",
    "    // The distances of the matches.\n",
    "    double distance = 2;\n",
    "  }\n",
    "  // All its neighbors.\n",
    "  repeated Neighbor neighbor = 1;\n",
    "}\n",
    "\n",
    "// Namespace specifies the rules for determining the datapoints that are\n",
    "// eligible for each matching query, overall query is an AND across namespaces.\n",
    "message Namespace {\n",
    "  // The string name of the namespace that this proto is specifying,\n",
    "  // such as \"color\", \"shape\", \"geo\", or \"tags\".\n",
    "  string name = 1;\n",
    "\n",
    "  // The allowed tokens in the namespace.\n",
    "  repeated string allow_tokens = 2;\n",
    "\n",
    "  // The denied tokens in the namespace.\n",
    "  // The denied tokens have exactly the same format as the token fields, but\n",
    "  // represents a negation. When a token is denied, then matches will be\n",
    "  // excluded whenever the other datapoint has that token.\n",
    "  //\n",
    "  // For example, if a query specifies {color: red, blue, !purple}, then that\n",
    "  // query will match datapoints that are red or blue, but if those points are\n",
    "  // also purple, then they will be excluded even if they are red/blue.\n",
    "  repeated string deny_tokens = 3;\n",
    "}\n",
    "Copied!\n",
    "Clone the repository that contains the dependencies of match_service.proto.\n",
    "! git clone https://github.com/googleapis/googleapis.git\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "Git Clone Result\n",
    "\n",
    "Compile the protocol buffer, that generates the following files: match_service_pb2.py and match_service_pb2_grpc.py.\n",
    "! python -m grpc_tools.protoc -I=. --proto_path=./googleapis --python_out=. --grpc_python_out=. match_service.proto\n",
    "Copied!\n",
    "Obtain the private Endpoint.\n",
    "DEPLOYED_INDEX_SERVER_IP = (\n",
    "    list(index_endpoint_client.list_index_endpoints(parent=PARENT))[0]\n",
    "    .deployed_indexes[0]\n",
    "    .private_endpoints.match_grpc_address\n",
    ")\n",
    "DEPLOYED_INDEX_SERVER_IP\n",
    "Copied!\n",
    "Test your query.\n",
    "import match_service_pb2\n",
    "import match_service_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel(\"{}:10000\".format(DEPLOYED_INDEX_SERVER_IP))\n",
    "stub = match_service_pb2_grpc.MatchServiceStub(channel)\n",
    "Copied!\n",
    "# Test query\n",
    "query = [\n",
    "    -0.11333,\n",
    "    0.48402,\n",
    "    0.090771,\n",
    "    -0.22439,\n",
    "    0.034206,\n",
    "    -0.55831,\n",
    "    0.041849,\n",
    "    -0.53573,\n",
    "    0.18809,\n",
    "    -0.58722,\n",
    "    0.015313,\n",
    "    -0.014555,\n",
    "    0.80842,\n",
    "    -0.038519,\n",
    "    0.75348,\n",
    "    0.70502,\n",
    "    -0.17863,\n",
    "    0.3222,\n",
    "    0.67575,\n",
    "    0.67198,\n",
    "    0.26044,\n",
    "    0.4187,\n",
    "    -0.34122,\n",
    "    0.2286,\n",
    "    -0.53529,\n",
    "    1.2582,\n",
    "    -0.091543,\n",
    "    0.19716,\n",
    "    -0.037454,\n",
    "    -0.3336,\n",
    "    0.31399,\n",
    "    0.36488,\n",
    "    0.71263,\n",
    "    0.1307,\n",
    "    -0.24654,\n",
    "    -0.52445,\n",
    "    -0.036091,\n",
    "    0.55068,\n",
    "    0.10017,\n",
    "    0.48095,\n",
    "    0.71104,\n",
    "    -0.053462,\n",
    "    0.22325,\n",
    "    0.30917,\n",
    "    -0.39926,\n",
    "    0.036634,\n",
    "    -0.35431,\n",
    "    -0.42795,\n",
    "    0.46444,\n",
    "    0.25586,\n",
    "    0.68257,\n",
    "    -0.20821,\n",
    "    0.38433,\n",
    "    0.055773,\n",
    "    -0.2539,\n",
    "    -0.20804,\n",
    "    0.52522,\n",
    "    -0.11399,\n",
    "    -0.3253,\n",
    "    -0.44104,\n",
    "    0.17528,\n",
    "    0.62255,\n",
    "    0.50237,\n",
    "    -0.7607,\n",
    "    -0.071786,\n",
    "    0.0080131,\n",
    "    -0.13286,\n",
    "    0.50097,\n",
    "    0.18824,\n",
    "    -0.54722,\n",
    "    -0.42664,\n",
    "    0.4292,\n",
    "    0.14877,\n",
    "    -0.0072514,\n",
    "    -0.16484,\n",
    "    -0.059798,\n",
    "    0.9895,\n",
    "    -0.61738,\n",
    "    0.054169,\n",
    "    0.48424,\n",
    "    -0.35084,\n",
    "    -0.27053,\n",
    "    0.37829,\n",
    "    0.11503,\n",
    "    -0.39613,\n",
    "    0.24266,\n",
    "    0.39147,\n",
    "    -0.075256,\n",
    "    0.65093,\n",
    "    -0.20822,\n",
    "    -0.17456,\n",
    "    0.53571,\n",
    "    -0.16537,\n",
    "    0.13582,\n",
    "    -0.56016,\n",
    "    0.016964,\n",
    "    0.1277,\n",
    "    0.94071,\n",
    "    -0.22608,\n",
    "    -0.021106,\n",
    "]\n",
    "Copied!\n",
    "request = match_service_pb2.MatchRequest()\n",
    "request.deployed_index_id = DEPLOYED_INDEX_ID\n",
    "for val in query:\n",
    "    request.float_val.append(val)\n",
    "\n",
    "# The output before stream update\n",
    "response = stub.Match(request)\n",
    "response\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "Match Response\n",
    "\n",
    "Insert datapoints.\n",
    "insert_datapoints_payload = aiplatform_v1.IndexDatapoint(\n",
    "    datapoint_id=\"101\",\n",
    "    feature_vector=query,\n",
    "    restricts=[{\"namespace\": \"class\", \"allow_list\": [\"101\"]}],\n",
    "    crowding_tag=aiplatform_v1.IndexDatapoint.CrowdingTag(crowding_attribute=\"b\"),\n",
    ")\n",
    "\n",
    "upsert_request = aiplatform_v1.UpsertDatapointsRequest(\n",
    "    index=INDEX_RESOURCE_NAME, datapoints=[insert_datapoints_payload]\n",
    ")\n",
    "\n",
    "index_client.upsert_datapoints(request=upsert_request)\n",
    "\n",
    "request = match_service_pb2.MatchRequest()\n",
    "request.deployed_index_id = DEPLOYED_INDEX_ID\n",
    "for val in query:\n",
    "    request.float_val.append(val)\n",
    "\n",
    "# The new inserted datapoint with id 101 will show up in the output\n",
    "response = stub.Match(request)\n",
    "response\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "Endpoint Response\n",
    "\n",
    "Add filtering.\n",
    "request = match_service_pb2.MatchRequest()\n",
    "request.deployed_index_id = DEPLOYED_INDEX_ID\n",
    "for val in query:\n",
    "    request.float_val.append(val)\n",
    "\n",
    "# Only the datapoints whose id is 1 and 101 will show up in the output\n",
    "restrict = match_service_pb2.Namespace()\n",
    "restrict.name = \"class\"\n",
    "restrict.allow_tokens.append(\"1\")\n",
    "restrict.allow_tokens.append(\"101\")\n",
    "\n",
    "request.restricts.append(restrict)\n",
    "\n",
    "response = stub.Match(request)\n",
    "response\n",
    "Copied!\n",
    "Update datapoint filtering:\n",
    "\n",
    "update_datapoints_payload = aiplatform_v1.IndexDatapoint(\n",
    "    datapoint_id=\"101\",\n",
    "    feature_vector=query,\n",
    "    restricts=[{\"namespace\": \"class\", \"allow_list\": [\"102\"]}],\n",
    "    crowding_tag=aiplatform_v1.IndexDatapoint.CrowdingTag(crowding_attribute=\"b\"),\n",
    ")\n",
    "\n",
    "upsert_request = aiplatform_v1.UpsertDatapointsRequest(\n",
    "    index=INDEX_RESOURCE_NAME, datapoints=[update_datapoints_payload]\n",
    ")\n",
    "\n",
    "index_client.upsert_datapoints(request=upsert_request)\n",
    "\n",
    "response = stub.Match(request)\n",
    "response\n",
    "Copied!\n",
    "Add crowding.\n",
    "request = match_service_pb2.MatchRequest()\n",
    "request.deployed_index_id = DEPLOYED_INDEX_ID\n",
    "for val in query:\n",
    "    request.float_val.append(val)\n",
    "\n",
    "# Set the limit of the number of neighbors in each crowding to 1\n",
    "# So no more than one neighbor of each crowding group will appear in the output\n",
    "request.per_crowding_attribute_num_neighbors = 1\n",
    "\n",
    "response = stub.Match(request)\n",
    "response\n",
    "Copied!\n",
    "Update datapoint crowding.\n",
    "# Change the crowding_attribute from 'b' to 'a' for the datapoint with id '101' by using stream update\n",
    "update_datapoints_payload = aiplatform_v1.IndexDatapoint(\n",
    "    datapoint_id=\"101\",\n",
    "    feature_vector=query,\n",
    "    restricts=[{\"namespace\": \"class\", \"allow_list\": [\"101\"]}],\n",
    "    crowding_tag=aiplatform_v1.IndexDatapoint.CrowdingTag(crowding_attribute=\"a\"),\n",
    ")\n",
    "\n",
    "upsert_request = aiplatform_v1.UpsertDatapointsRequest(\n",
    "    index=INDEX_RESOURCE_NAME, datapoints=[update_datapoints_payload]\n",
    ")\n",
    "\n",
    "index_client.upsert_datapoints(request=upsert_request)\n",
    "\n",
    "response = stub.Match(request)\n",
    "response\n",
    "Copied!\n",
    "Remove datapoints.\n",
    "# Remove the datapoint with id '101' from the index\n",
    "remove_request = aiplatform_v1.RemoveDatapointsRequest(\n",
    "    index=INDEX_RESOURCE_NAME, datapoint_ids=[\"101\"]\n",
    ")\n",
    "\n",
    "index_client.remove_datapoints(request=remove_request)\n",
    "\n",
    "request = match_service_pb2.MatchRequest()\n",
    "request.deployed_index_id = DEPLOYED_INDEX_ID\n",
    "for val in query:\n",
    "    request.float_val.append(val)\n",
    "\n",
    "response = stub.Match(request)\n",
    "response\n",
    "Copied!\n",
    "Output:\n",
    "\n",
    "Remove Datapoint Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e39611",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb9c82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad5011",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dd242",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85500379",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
