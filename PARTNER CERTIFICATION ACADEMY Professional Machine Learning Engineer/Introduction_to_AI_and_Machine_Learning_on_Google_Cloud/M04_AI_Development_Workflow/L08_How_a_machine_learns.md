# How_a_machine_learns

# ğŸ§  **Understanding How Neural Networks Learn**

## ğŸ” **Overview**  
To understand **machine learning**, you must first explore **how neural networks learn**. This includes:  
- The **learning process**  
- Key **terms and concepts**  
- The **role of activation functions, loss functions, and gradient descent**  

If you're already familiar with **ML theories and terminologies**, feel free to **skip this lesson**.

---

## âš™ï¸ **How Do Machines Learn?**
Neural networks are the foundation of machine learning. You may have heard of different types:  
- **Artificial Neural Networks (ANNs)**  
- **Deep Neural Networks (DNNs)**  
- **Convolutional Neural Networks (CNNs)**  
- **Recurrent Neural Networks (RNNs)**  
- **Large Language Models (LLMs)**  

All of these originate from the **basic ANN**, which consists of:  
1ï¸âƒ£ **Input Layer**  
2ï¸âƒ£ **Hidden Layer(s)**  
3ï¸âƒ£ **Output Layer**  

Each **neuron** (node) connects to others via **weights**, which the **network learns** during training.

---

## ğŸ”¢ **How Neural Networks Process Data**
1ï¸âƒ£ **Calculate the weighted sum** of inputs.  
2ï¸âƒ£ **Apply an activation function** to introduce **non-linearity**.  
3ï¸âƒ£ **Pass the output to the next layer** until reaching the final prediction.  
4ï¸âƒ£ **Compare the predicted output (Å·) to the actual result (y)** using a **loss function**.  
5ï¸âƒ£ **Backpropagation** adjusts **weights** based on the loss.  
6ï¸âƒ£ **Gradient descent** helps **optimize the learning process**.  
7ï¸âƒ£ **Iteration continues** until the model reaches an **optimal state**.

---

## ğŸš€ **Activation Functions: Making Neural Networks Non-Linear**
Without activation functions, a **neural network remains linear** and lacks **complex decision-making capabilities**.  
Common activation functions:  
- **ReLU (Rectified Linear Unit)** â†’ Outputs **0** for negative values, keeps **positive** values unchanged.  
- **Sigmoid** â†’ Maps output to a **range between 0 and 1** (good for **binary classification**).  
- **Tanh (Hyperbolic Tangent)** â†’ Maps values between **-1 and 1**, providing **stronger gradients**.  
- **Softmax** â†’ Converts outputs into a **probability distribution**, used for **multi-class classification**.

---

## ğŸ“‰ **Loss Functions: Measuring Errors**
Loss functions evaluate **how far the model's predictions are from the actual values**.  
- **Mean Squared Error (MSE)** â†’ Common in **regression** problems.  
- **Cross-Entropy Loss** â†’ Used in **classification** problems.  

ğŸ“Œ **Loss function vs. Cost function**:  
- **Loss function** â†’ Measures error **for a single prediction**.  
- **Cost function** â†’ Measures error **for the entire dataset**.

---

## ğŸ”ï¸ **Gradient Descent: Optimizing Learning**
To **minimize error**, neural networks use **gradient descent** to adjust weights.  
1ï¸âƒ£ **Compute the derivative** of the loss function.  
2ï¸âƒ£ **Move in the opposite direction** of the gradient to minimize loss.  
3ï¸âƒ£ **Repeat** until reaching the lowest possible error.  

ğŸ“Œ **Learning Rate (Step Size)**:  
- **Too small** â†’ Learning is **slow**.  
- **Too large** â†’ Model may **fail to converge**.  
- **Optimal learning rate** ensures **efficient training**.

---

## ğŸ”„ **Training Iterations & Hyperparameters**
A **complete pass through the dataset** is called an **epoch**.  
- **More epochs** â†’ Better learning, but **risk of overfitting**.  
- **Fewer epochs** â†’ **Underfitting**, where the model **fails to learn patterns**.  

### **Key Training Terms**
ğŸ”¹ **Weights & Biases** â†’ Adjusted by the model during learning.  
ğŸ”¹ **Hyperparameters** â†’ Defined **before training** (e.g., number of layers, learning rate, activation functions).  
ğŸ”¹ **Backpropagation** â†’ **Adjusts weights** to minimize error.  
ğŸ”¹ **AutoML** â†’ **Automates hyperparameter tuning**, saving experimentation time.

---

## ğŸ¯ **Key Takeaways**
- Neural networks **learn by adjusting weights** to minimize error.  
- **Activation functions** introduce **non-linearity**, enabling **complex problem-solving**.  
- **Loss functions** measure **how well the model is performing**.  
- **Gradient descent** helps **optimize** learning.  
- **Hyperparameters** (e.g., learning rate, number of layers) **must be tuned for optimal performance**.  
- **AutoML** can automate **many of these processes**.  

Youâ€™ll revisit these concepts in **upcoming lessons and labs**! ğŸš€  
