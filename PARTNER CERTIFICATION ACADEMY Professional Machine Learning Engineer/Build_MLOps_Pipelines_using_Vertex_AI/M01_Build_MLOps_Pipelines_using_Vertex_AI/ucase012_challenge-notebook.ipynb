{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb0f10c-6ce4-46ba-aa5f-bddde86e7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f494d8d-ce74-43b4-ab4b-317b71964aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install {USER_FLAG} google-cloud-aiplatform --upgrade\n",
    "!pip3 install {USER_FLAG} google-cloud-pipeline-components\n",
    "!pip3 install {USER_FLAG} fsspec gcsfs scikit-learn\n",
    "!pip3 install -U google-cloud-aiplatform \"shapely>2\"\n",
    "!pip3 install docstring-parser\n",
    "!pip3 install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c94e8b-bf96-43ec-ad94-88af0701b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f80ef-c757-4c0e-a660-c00dc17a8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe2697-1ad1-4130-af87-0152c7a54d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9607c-4b4e-4b69-a1e7-30834033d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=\"gs://\" + PROJECT_ID + \"-bucket\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4fc70-c705-4ada-97ae-aeb08880dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/partner-usecase-bucket/ucase012/Bike-Sharing-Dataset.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5eb9e-8fd9-4aee-ac81-e056570efa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q Bike-Sharing-Dataset.zip -d data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256eb7b4-7176-4fe8-8db8-6feae68103ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp ./data/hour.csv $BUCKET_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d61ebb-a8ea-45b6-9665-33be366c7a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645ffea-208b-47b3-b729-667c1b96031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint as pp\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error,mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf12bf-52f4-4448-bf22-2e0b929f0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = BUCKET_NAME+\"/hour.csv\"\n",
    "data = pd.read_csv(url)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d898648-9954-4a1a-ba72-5e7077a100b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_columns = [\"season\" \n",
    "                     , \"yr\" \n",
    "                     ,\"mnth\" \n",
    "                     ,\"hr\" \n",
    "                     ,\"holiday\" \n",
    "                     , \"weekday\" \n",
    "                     , \"workingday\" \n",
    "                     , \"weathersit\" \n",
    "                     , \"temp\" \n",
    "                     , \"atemp\" \n",
    "                     , \"hum\" \n",
    "                     , \"windspeed\" \n",
    "                     , \"casual\" \n",
    "                     , \"registered\" \n",
    "                     , \"cnt\"\n",
    "                    ]\n",
    "data = data[modelling_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f8eb7-d07e-4fda-b3a2-4d1be5e9b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "test_size = 0.1\n",
    "valid_size = 0.1\n",
    "\n",
    "train_ds, valid_ds, test_ds = np.split(data.sample(frac=1, random_state=42), [int((train_size)*len(data)), int((1-test_size)*len(data))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4b98e-4fe8-43c2-81c3-39ba23d07bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"cnt\"\n",
    "\n",
    "x_train = train_ds.drop(columns=target, axis=1)\n",
    "y_train = train_ds[target]\n",
    "\n",
    "x_valid = valid_ds.drop(columns=target, axis=1)\n",
    "y_valid = valid_ds[target]\n",
    "\n",
    "x_test = test_ds.drop(columns=target, axis=1)\n",
    "y_test = test_ds[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360fd733-2cc5-45cf-9a79-19b61c2a1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(x_train , y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d71389-be70-47e6-b354-ee572df62876",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_valid)\n",
    "\n",
    "#evaluate Model\n",
    "adj_r2 = r2_score(y_true=y_valid, y_pred=y_pred)\n",
    "mae = mean_absolute_error(y_true=y_valid, y_pred=y_pred)\n",
    "mse = mean_squared_error(y_true=y_valid, y_pred=y_pred)\n",
    "mape = mean_absolute_percentage_error(y_true=y_valid, y_pred=y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Adjusted R2 : {adj_r2}\")\n",
    "print(f\"Mean Absolute Error : {mae}\")\n",
    "print(f\"Mean Absolute Percentage Error : {round(mape,4)*100}%\")\n",
    "print(f\"Mean Squared Error : {mse}\")\n",
    "print(f\"Root Mean Squared Error : {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193ad29-fe9a-4e0e-bc2a-a2a1cd22607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH=BUCKET_NAME+\"/models/\"\n",
    "model_path = \"./\" + \"model.pkl\"\n",
    "with open(model_path, 'wb') as file:  \n",
    "    pickle.dump(model, file) \n",
    "    \n",
    "#copy model artifacts to GCS storage\n",
    "!gsutil cp \"model.pkl\" $MODEL_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab8929c-179d-4bca-a45e-c4e04a83f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE TO UPLOAD THE MODEL TO THE REGISTRY BELOW. USE THE FOLLOWING FOR THE VARIOUS PARAMETERS REQUIRED BY THE KFP SDK TO UPLOAD A MODEL:\n",
    "\n",
    "container to use: \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\"\n",
    "name: \"MLOps0-model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61793949-c12e-4025-8c68-2f3dc99700ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Prediction containers list available at : https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "# serving_container_uri = \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\"\n",
    "\n",
    "# --== SOLUTION BELOW ==--\n",
    "\n",
    "# #define GCS location for model artifacts\n",
    "# artifact_uri = MODEL_PATH\n",
    "\n",
    "# #Upload Model to Vertex AI Model Registry using Python SDK\n",
    "# model = aiplatform.Model.upload(display_name= \"MLOps0-model\" ,\n",
    "#                                     artifact_uri=artifact_uri,\n",
    "#                                     serving_container_image_uri=serving_container_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c53178e-90b7-448b-84ea-67936fc6359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE TO DEPLOY THE ENDPOINT BELOW. USE THE FOLLOWING FOR THE VARIOUS PARAMETERS REQUIRED BY THE KFP SDK TO DEPLOY THE ENDPOINT.\n",
    "machine type to use: \"e2-standard-2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf665283-258b-40ef-b4e1-ab4d8b0120d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --== SOLUTION BELOW ==--\n",
    "\n",
    "# #Create the model endpoint using Python SDK\n",
    "# endpoint = model.deploy(machine_type=\"n1-standard-4\",\n",
    "#                         min_replica_count=1,\n",
    "#                         max_replica_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd33ac5-36da-4cbc-ae80-25ede2a1d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the model endpoint using Python SDK\n",
    "\n",
    "#create list to hold request data\n",
    "instances = [\n",
    "    [1.0, 0.0, 1.0, 0.0, 0.0, 6.0, 0.0, 1.0, 0.24, 0.2879, 0.81, 0.0, 3.0, 13.0],\n",
    "  ]\n",
    "\n",
    "prediction = endpoint.predict(instances=instances)\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fea83c-aed6-4c08-9727-2a18c4b399b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undeploy the model and delete the endpoint\n",
    "endpoint.undeploy_all()\n",
    "endpoint.delete()\n",
    "model.delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20323463-c1d7-41a8-8992-e6ab35c7e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component , ClassificationMetrics , Metrics)\n",
    "\n",
    "from kfp import compiler\n",
    "from kfp.components import load_component_from_file\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e5f0e-d903-40cf-87ce-fbe10435470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d60a67-a7e9-40dd-81b9-0ef109ab1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\" , \"fsspec\" , \"gcsfs\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"./components/download_data.yaml\"\n",
    ")\n",
    "def download_data(input_data_path : str\n",
    "                  , input_data_filename : str\n",
    "                  , downloaded_data : Output[Dataset]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    print(f\"input_data_path : {input_data_path}\")\n",
    "    print(f\"input_data_filename : {input_data_filename}\")\n",
    "    print(f\"downloaded_data : {downloaded_data}\")\n",
    "    print(f\"downloaded_data.path : {downloaded_data.path}\")\n",
    "                \n",
    "    url = os.path.join(input_data_path , input_data_filename)\n",
    "    \n",
    "    #read data from GCS location\n",
    "    data = pd.read_csv(url)\n",
    "    \n",
    "    #write to output dataset path\n",
    "    output_data_uri = downloaded_data.path + \".csv\" \n",
    "    data.to_csv(output_data_uri \n",
    "                , index=False\n",
    "                , encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36480cb0-c5ba-44a2-b2f1-1afbb6ae1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"fsspec\" , \"gcsfs\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"./components/preprocess_data.yaml\"\n",
    ")\n",
    "def preprocess_data(train_size : float \n",
    "                    , test_size : float\n",
    "                    , valid_size : float\n",
    "                    , train_data : Output[Dataset]\n",
    "                    , valid_data : Output[Dataset]\n",
    "                    , test_data : Output[Dataset]\n",
    "                    , input_data:  Input[Dataset]\n",
    "                   ):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(f\"train_size : {train_size}\")\n",
    "    print(f\"test_size : {test_size}\")\n",
    "    print(f\"valid_size : {valid_size}\")\n",
    "    print(f\"train_data : {train_data}\")\n",
    "    print(f\"valid_data : {valid_data}\")\n",
    "    print(f\"test_data : {test_data}\")\n",
    "    print(f\"input_data : {input_data}\")\n",
    "    \n",
    "    data = pd.read_csv(input_data.path + \".csv\")\n",
    "    \n",
    "    modelling_columns = [\"season\" \n",
    "                     , \"yr\" \n",
    "                     ,\"mnth\" \n",
    "                     ,\"hr\" \n",
    "                     ,\"holiday\" \n",
    "                     , \"weekday\" \n",
    "                     , \"workingday\" \n",
    "                     , \"weathersit\" \n",
    "                     , \"temp\" \n",
    "                     , \"atemp\" \n",
    "                     , \"hum\" \n",
    "                     , \"windspeed\" \n",
    "                     , \"casual\" \n",
    "                     , \"registered\" \n",
    "                     , \"cnt\"\n",
    "                    ]\n",
    "    \n",
    "    data = data[modelling_columns]\n",
    "\n",
    "    train_ds, valid_ds, test_ds = np.split(data.sample(frac=1, random_state=42), [int((train_size)*len(data)), int((1-test_size)*len(data))])\n",
    "    train_ds.to_csv(train_data.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    valid_ds.to_csv(valid_data.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    test_ds.to_csv(test_data.path + \".csv\" , index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7bc0ff-d6a2-4dd9-bbe2-b7dbbe61d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\",  \"scikit-learn==1.3.2\" , \"fsspec\" , \"gcsfs\"]\n",
    "    , base_image=\"python:3.9\"\n",
    "    , output_component_file=\"./components/train.yaml\"\n",
    ")\n",
    "def train_model(\n",
    "    train_data:  Input[Dataset],\n",
    "    model: Output[Model], \n",
    "):\n",
    "    \n",
    "    print(f\"train_data : {train_data}\")\n",
    "    print(f\"model : {model}\")\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import sklearn\n",
    "\n",
    "    train_ds = pd.read_csv(train_data.path+\".csv\")\n",
    "    my_model = RandomForestRegressor()\n",
    "    \n",
    "    target = \"cnt\"\n",
    "    \n",
    "    x_train = train_ds.drop(columns=target, axis=1)\n",
    "    y_train = train_ds[target]\n",
    "    \n",
    "    my_model.fit(x_train , y_train)\n",
    "    model.metadata[\"model_name\"] = \"RandomForestRegressor\"\n",
    "    model.metadata[\"framework\"] = \"sklearn\"\n",
    "    model.metadata[\"framework_version\"] = sklearn.__version__\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    \n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(my_model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8598826f-32e9-4bd9-99ae-291a6fff33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\",  \"scikit-learn==1.3.2\" , \"fsspec\" , \"gcsfs\"]\n",
    "    , base_image=\"python:3.9\"\n",
    "    , output_component_file=\"./components/evaluate_model.yaml\"\n",
    ")\n",
    "def evaluate_model(\n",
    "    test_data:  Input[Dataset],\n",
    "    model: Input[Model], \n",
    "    target_column_name : str ,\n",
    "    deployment_metric : str ,\n",
    "    deployment_metric_threshold : float ,\n",
    "    kpi: Output[Metrics]\n",
    ")-> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"deploy_flag\", str),  # Return parameter.\n",
    "    ],\n",
    "):\n",
    "    \n",
    "    print(f\"test_data : {test_data}\")\n",
    "    print(f\"model : {model}\")\n",
    "    print(f\"kpi : {kpi}\")\n",
    "    print(f\"deployment_metric : {deployment_metric}\")\n",
    "    print(f\"deployment_metric_threshold : {deployment_metric_threshold}\")\n",
    "    \n",
    "    from sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error,mean_absolute_percentage_error\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import json\n",
    "    \n",
    "    test_ds = pd.read_csv(test_data.path+\".csv\")\n",
    "    target = target_column_name\n",
    "    \n",
    "    x_test = test_ds.drop(columns=target, axis=1)\n",
    "    y_test = test_ds[target]\n",
    "    \n",
    "    print(f\"model.path : {model.path}\")\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    print(f\"file_name : {file_name}\")\n",
    "    #model = pickle.loads(file_name)\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        model = pickle.load(file)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    r2 = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "    mae = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
    "    mse = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true=y_test, y_pred=y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    model_metrics = {\"r2\" : r2 \n",
    "                     , \"mae\" : mae \n",
    "                     , \"mape\" : mape \n",
    "                     , \"mse\" : mse \n",
    "                     , \"rmse\" : rmse\n",
    "                    }\n",
    "    \n",
    "    print(f\"Adjusted_R2 : {r2}\")\n",
    "    print(f\"Mean Absolute Error : {mae}\")\n",
    "    print(f\"Mean Absolute Percentage Error : {round(mape,4)*100}%\")\n",
    "    print(f\"Mean Squared Error : {mse}\")\n",
    "    print(f\"Root Mean Squared Error : {rmse}\")\n",
    "    \n",
    "    kpi.log_metric(\"Adjusted_R2\", float(r2))\n",
    "    kpi.log_metric(\"Mean Absolute Error\", float(mae))\n",
    "    kpi.log_metric(\"Mean Absolute Percentage Error\", float(mape))\n",
    "    kpi.log_metric(\"Mean Squared Error\", float(mse))\n",
    "    kpi.log_metric(\"Root Mean Squared Error\", float(rmse))\n",
    "    \n",
    "    actual_metric_value = model_metrics.get(deployment_metric)\n",
    "    \n",
    "    if actual_metric_value >= deployment_metric_threshold:\n",
    "        deploy_flag = \"True\"\n",
    "    else:\n",
    "        deploy_flag = \"False\"\n",
    "        \n",
    "    return (deploy_flag,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31411df-a274-48f0-8f65-1b46e935e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\",  \"scikit-learn==1.3.2\" , \"fsspec\" , \"gcsfs\" , \"google-cloud-aiplatform\"]\n",
    "    , base_image=\"python:3.9\"\n",
    "    , output_component_file=\"./components/register_model.yaml\"\n",
    ")\n",
    "def register_model(\n",
    "    serving_container_uri : str ,\n",
    "    project_id : str ,\n",
    "    region: str,\n",
    "    model_name : str , \n",
    "    model: Input[Model], \n",
    ")-> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"model_resource_name\", str),  # Return parameter.\n",
    "    ],\n",
    "):\n",
    "    \n",
    "    print(f\"serving_container_uri : {serving_container_uri}\")\n",
    "    print(f\"project_id : {project_id}\")\n",
    "    print(f\"region : {region}\")\n",
    "    print(f\"model : {model}\")\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    print(f\"model.uri : {model.uri[:-5]}\")\n",
    "    \n",
    "    aiplatform.init(project = project_id , location=region)\n",
    "    model = aiplatform.Model.upload(display_name= model_name ,\n",
    "                                    artifact_uri=model.uri[:-5],\n",
    "                                    serving_container_image_uri=serving_container_uri)\n",
    "    return (model.resource_name,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f2a594-6a6c-43a5-9fc2-205efc307e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\",  \"scikit-learn==1.3.2\" , \"fsspec\" , \"gcsfs\", \"google-cloud-aiplatform\"]\n",
    "    , base_image=\"python:3.9\"\n",
    "    , output_component_file=\"./components/deploy_model.yaml\"\n",
    ")\n",
    "def deploy_model(\n",
    "    model_resource_name : str ,\n",
    "    project_id : str ,\n",
    "    region: str\n",
    ")-> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"endpoint_resource_name\", str),  # Return parameter.\n",
    "    ],\n",
    "):\n",
    "    \n",
    "    print(f\"model_resource_name : {model_resource_name}\")\n",
    "    print(f\"project_id : {project_id}\")\n",
    "    print(f\"region : {region}\")\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    aiplatform.init(project = project_id , location=region)\n",
    "    \n",
    "    model = aiplatform.Model(model_resource_name)\n",
    "    endpoint = model.deploy(machine_type=\"n1-standard-4\",\n",
    "                        min_replica_count=1,\n",
    "                        max_replica_count=1)\n",
    "    \n",
    "    return (endpoint.resource_name,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bb867-3afb-4023-b399-156f16b2c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_output = !gcloud auth list 2>/dev/null\n",
    "SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)\n",
    "print(\"Project ID: \", PROJECT_ID)\n",
    "print(\"staging_bucket_uri: \",BUCKET_NAME)\n",
    "print(\"input_data_path: \",BUCKET_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5837901e-3f33-45b5-9c26-f2534ba181c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"project\":\"ENTER PROJECT_ID HERE\",\n",
    "    \"region\":\"us-central1\",\n",
    "    \"service_account\":\"ENTER SERVICE_ACCOUNT HERE\",\n",
    "    \"staging_bucket_uri\":\"ENTER BUCKET_NAME HERE\",\n",
    "    \"pipeline_name\":\"tabular-data-regression-kfp-cicd-pipeline\",\n",
    "    \"pipeline_package_path\":\"tabular-data-regression-kfp-cicd-pipeline.json\",\n",
    "    \"input_data_path\":\"ENTER BUCKET_NAME HERE\",\n",
    "    \"input_data_filename\":\"hour.csv\",\n",
    "    \"target_column_name\":\"cnt\",\n",
    "    \"train_size\":0.8,\n",
    "    \"test_size\":0.1,\n",
    "    \"valid_size\":0.1,\n",
    "    \"deployment_metric\":\"r2\",\n",
    "    \"deployment_metric_threshold\":0.8,\n",
    "    \"model_name\":\"model_tabular_regression\",\n",
    "    \"serving_container_uri\":\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e0467-7a72-4cd7-a08e-fa1c2bf17bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build_pipeline.py\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from kfp.components import load_component_from_file\n",
    "\n",
    "download_data = load_component_from_file(\"./components/download_data.yaml\")\n",
    "preprocess_data = load_component_from_file(\"./components/preprocess_data.yaml\")\n",
    "train_model = load_component_from_file(\"./components/train.yaml\")\n",
    "evaluate_model = load_component_from_file(\"./components/evaluate_model.yaml\")\n",
    "register_model = load_component_from_file(\"./components/register_model.yaml\")\n",
    "deploy_model = load_component_from_file(\"./components/deploy_model.yaml\")\n",
    "\n",
    "#read configuration from file\n",
    "with open(\"config.json\") as json_file:\n",
    "    config = json.load(json_file)\n",
    "    \n",
    "PIPELINE_NAME = config.get(\"pipeline_name\")\n",
    "PACKAGE_PATH = config.get(\"pipeline_package_path\")\n",
    "BUCKET_URI = config.get(\"staging_bucket_uri\")\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/kfp_tabular_data_regression\".format(BUCKET_URI)\n",
    "print(f\"PIPELINE_ROOT :{PIPELINE_ROOT}\")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to define the pipeline Context.\n",
    "    name=PIPELINE_NAME,\n",
    "    \n",
    ")\n",
    "def pipeline(project: str = \"\",\n",
    " region: str = \"\",\n",
    " service_account: str = \"\",\n",
    " staging_bucket_uri: str = \"\",\n",
    " pipeline_name: str = \"\",\n",
    " pipeline_package_path: str = \"\",\n",
    " input_data_path: str = \"\",\n",
    " input_data_filename: str = \"\",\n",
    " target_column_name: str = \"\",\n",
    " train_size: float = 0.8,\n",
    " test_size: float = 0.1,\n",
    " valid_size: float = 0.1,\n",
    " hypertune_container_image_uri: str = \"\" ,\n",
    " hypertune_machine_type: str = \"\",\n",
    " hypertune_machine_replica_count: int = 1 ,\n",
    " hypertune_max_trial_count: int = 1 ,\n",
    " hypertune_parallel_trial_count: int = 1 ,\n",
    " hypertune_metric : str = \"\" ,\n",
    " hypertune_metric_objective : str = \"\" ,\n",
    " hypertune_job_name: str = \"\" ,\n",
    " deployment_metric: str = \"\" ,\n",
    " deployment_metric_threshold: float = 0.8 ,\n",
    " serving_container_uri : str = \"\" ,\n",
    " model_name : str = \"\", \n",
    " user_email : str = \"\",\n",
    " monitoring_job_name : str = \"\" ,\n",
    " predict_instance_schema_uri : str = \"\"\n",
    "):\n",
    "    \n",
    "    download_data_op = download_data(input_data_path = input_data_path \n",
    "                                     , input_data_filename = input_data_filename\n",
    "                                    )\n",
    "    \n",
    "    \n",
    "    preprocess_data_op = preprocess_data(train_size = train_size\n",
    "                                         , test_size = test_size\n",
    "                                         , valid_size = valid_size\n",
    "                                         , input_data = download_data_op.outputs[\"downloaded_data\"])\n",
    "    \n",
    "    train_model_op = train_model(train_data = preprocess_data_op.outputs[\"train_data\"])\n",
    "    \n",
    "    \n",
    "    evaluate_model_op = evaluate_model(test_data = preprocess_data_op.outputs[\"test_data\"]\n",
    "                                       ,model = train_model_op.outputs[\"model\"]\n",
    "                                       ,target_column_name = target_column_name\n",
    "                                       ,deployment_metric = deployment_metric \n",
    "                                       ,deployment_metric_threshold = deployment_metric_threshold\n",
    "                                      )\n",
    "    \n",
    "    with dsl.If(evaluate_model_op.outputs[\"deploy_flag\"] == \"True\"):\n",
    "        \n",
    "        register_model_op = register_model(serving_container_uri = serving_container_uri \n",
    "                                       , model = train_model_op.outputs[\"model\"]\n",
    "                                       , model_name = model_name\n",
    "                                       , project_id = project \n",
    "                                       , region = region)\n",
    "        \n",
    "        #deploy only if metric value exceeds deployment threshold\n",
    "        deploy_model_op = deploy_model(model_resource_name = register_model_op.outputs[\"model_resource_name\"] \n",
    "                                   , project_id = project \n",
    "                                   , region = region)\n",
    "    \n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline\n",
    "    , package_path=PACKAGE_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647ac32-9f08-43dc-82a5-67954a42c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_pipeline.py\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "with open(\"config.json\") as json_file:\n",
    "    config = json.load(json_file)\n",
    "    \n",
    "SERVICE_ACCOUNT = config.get(\"service_account\")\n",
    "DISPLAY_NAME = config.get(\"pipeline_name\")\n",
    "PACKAGE_PATH = config.get(\"pipeline_package_path\")\n",
    "BUCKET_URI = config.get(\"staging_bucket_uri\")\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/kfp_tabular_data_regression\".format(BUCKET_URI)\n",
    "print(f\"PIPELINE_ROOT :{PIPELINE_ROOT}\")\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=PACKAGE_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values=config,\n",
    ")\n",
    "\n",
    "job.submit(service_account = SERVICE_ACCOUNT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cebe0d-9b6b-4935-bd38-4330e90d365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 build_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6df574-3eae-479b-af49-f2d2a09d28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 run_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85142364-7134-49a0-b266-b699e05b4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp ./config.json $BUCKET_NAME\n",
    "! gsutil cp ./tabular-data-regression-kfp-cicd-pipeline.json $BUCKET_NAME\n",
    "! gsutil acl ch -u AllUsers:R $BUCKET_NAME/config.json\n",
    "! gsutil acl ch -u AllUsers:R $BUCKET_NAME/tabular-data-regression-kfp-cicd-pipeline.json"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "tf2-gpu.2-15.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m124"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] * (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
